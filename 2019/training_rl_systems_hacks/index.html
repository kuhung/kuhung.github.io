<!DOCTYPE html>
<html lang="zh-CN">
  <head>
  <meta http-equiv="content-type" content="text/html;charset=utf-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="robots" content="noodp"/>
  <meta name="author" content="谷粒">
  <meta name="description" content="数据挖掘/机器学习工程师">
  <meta name="keywords" content="华中科技大学,谷粒,谷粒粒,机器学习,社交网络，数据挖掘,网易游戏,米哈游,广告投放,用户分析,增长黑客,开发者,极客,代码,开源,Developer,Programmer,Coder,Geek,DataScientist">
  
  
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-76017508-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
  <link rel="prev" href="https://kuhungio.me/2019/flask_vue_ml/" />
  <link rel="next" href="https://kuhungio.me/2019/book-list-for-ds/" />
  <link rel="canonical" href="https://kuhungio.me/2019/training_rl_systems_hacks/" />
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">
  <title>
       
       
           深度强化学习技巧 hacks for training deep RL | 谷粒的博客
       
  </title>
  <meta name="title" content="深度强化学习技巧 hacks for training deep RL | 谷粒的博客">
    
  
  <link rel="stylesheet" href="/font/iconfont.css">
  <link rel="stylesheet" href="/css/main.min.css">


  
  
 

<script type="application/ld+json">
 "@context" : "http://schema.org",
    "@type" : "BlogPosting",
    "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "https:\/\/kuhungio.me"
    },
    "articleSection" : "posts",
    "name" : "深度强化学习技巧 hacks for training deep RL",
    "headline" : "深度强化学习技巧 hacks for training deep RL",
    "description" : "深度强化学习技巧 hacks for training deep RL 这是一篇旧文，John Schulman 《深度增强学习研究基础》演讲(Aug 2017)中记录的 tricks。近日重看，发现有些东西在工程中是通用的，值得一读。 测试新算法的技巧 简化问题，使用低维变量。 使用类似只有角度和速度两个变量的 Pendulum problem 问题。 这样做方便将目标函数、算法的最终状态以及算法的迭代情况可视化出来。 当出现问题时，更容易将出问题的点直观的表达（比如目标函数是否够平滑等问题）。 构造一个 demo 来测试你的算法 比如：对于一个分层强化学习算法，你应该构造一个算法可以直观学习到分层的问题。 这样能够轻易地发现那里出了问题。 注意：不要在这样的小问题上过分的尝试。 在熟悉的场景中测试 随着时间的推移，你将能预估训练所需的时间。 明白你的奖赏是如何变化的。 能够设定一个基线，以便让你知道相对过去改进了多少。 作者使用他的 hpper robot，因为他知道算法应该学多块，以及哪些行为是异常的。 快速上手新任务的技巧 简化问题 从简单的开始，直到回到问题。 途径1： 简化特征空间 举例来说，如果你是想从图片（高维空间）中学习，那么你可能先需要处理特征。举个例子：如果你的算法是想标定某个事物的位置，一开始，使用单一的x，y坐标可能会更好。 一旦起步，逐步还原问题直到解决问题。 途径2：简化奖赏函数 简化奖赏函数，这样可以有一个更快的反馈，帮助你知道是不是走偏了。 比如：击中时给 robot 记一分。这种情况很难学习，因为在开始于奖赏之前有太多的可能。将击中得分改为距离，这样将提升学习速率、更快迭代。 将一个问题转化为强化学习的技巧 可能现实是并不清楚特征是什么，也不清楚奖赏该是什么。或者，问题是什么都还不清楚。\n第一步：将这个问题使用随机策略可视化出来。 看看那些部分吸引了你。\n如果这个随机策略在某些情况下做了正确的事，那么很大概率，强化学习也可以做到。\n策略的更新将会发现这里面的行为，并促使稳定下来。 如果随机策略永远都做不到，那么强化学习也不可能。 确保可观测 确保你能够掌控系统，且给 agent 的也是同样的系统环境。 举个例子： 亲自查看处理过图片，以确保你没有移出掉关键信息或者是在某种程度上阻碍算法。 确保所有的事物都在合理的尺度 经验法则： 观测环境： 确保均值为0，方差为1。 奖赏： 如果你能控制它，就把他缩放到一个合理的维度。 在所有的数据上都做同样的处理。 检查所有的观测环境以及奖赏，以确保没有特别离奇的异常值。 建立一个好的基线 一开始并不清楚哪些算法会起作用，所以设定一系列的基线（从其他方法）。 交叉熵 策略更新 一些类型的 Q-learning 算法: OpenAI Baselines 或者 RLLab 复现论文 某些时候（经常的事），复现论文结果特别困难。有如下一些技巧：",
    "inLanguage" : "zh-CN",
    "author" : "谷粒",
    "creator" : "谷粒",
    "publisher": "谷粒",
    "accountablePerson" : "谷粒",
    "copyrightHolder" : "谷粒",
    "copyrightYear" : "2019",
    "datePublished": "2019-05-02 11:59:48 \u002b0800 CST",
    "dateModified" : "2019-05-02 11:59:48 \u002b0800 CST",
    "url" : "https:\/\/kuhungio.me\/2019\/training_rl_systems_hacks\/",
    "wordCount" : "266",
    "keywords" : [ "强化学习","深度学习", "谷粒的博客"]
}
</script>

</head>

  


  <body class="">
    <div class="wrapper">
        <nav class="navbar">
    <div class="container">
        <div class="navbar-header header-logo">
        	<a href="javascript:void(0);" class="theme-switch"><i class="iconfont icon-xihuan"></i></a>&nbsp;<a href="https://kuhungio.me">谷粒的博客</a>
        </div>
        <div class="menu navbar-right">
                
                
                <a class="menu-item" href="/posts/" title="">历史文章</a>
                
                <a class="menu-item" href="/tags/mindmap/" title="">思维导图</a>
                
                <a class="menu-item" href="/about/" title="关于我">关于我</a>
                
        </div>
    </div>
</nav>
<nav class="navbar-mobile" id="nav-mobile" style="display: none">
     <div class="container">
        <div class="navbar-header">
            <div>  <a href="javascript:void(0);" class="theme-switch"><i class="iconfont icon-xihuan"></i></a>&nbsp;<a href="https://kuhungio.me">谷粒的博客</a></div>
            <div class="menu-toggle">
                <span></span><span></span><span></span>
            </div>
        </div>
     
          <div class="menu" id="mobile-menu">
                
                
                <a class="menu-item" href="/posts/" title="">历史文章</a>
                
                <a class="menu-item" href="/tags/mindmap/" title="">思维导图</a>
                
                <a class="menu-item" href="/about/" title="关于我">关于我</a>
                
        </div>
    </div>
</nav>
    	 <main class="main">
          <div class="container">
      		
<article class="post-warp" itemscope itemtype="http://schema.org/Article">
    <header class="post-header">
        <h1 class="post-title" itemprop="name headline">深度强化学习技巧 hacks for training deep RL</h1>
        <div class="post-meta">
                Written by <a itemprop="name" href="https://kuhungio.me" rel="author">谷粒</a> with ♥
                <span class="post-time">
                on <time datetime=2019-05-02 itemprop="datePublished">May 2, 2019</time>
                </span>
                in
                <i class="iconfont icon-folder"></i>
                <span class="post-category">
                        <a href="https://kuhungio.me/categories/%E6%8A%80%E6%9C%AF%E6%8A%80%E5%B7%A7/"> 技术技巧 </a>
                        <a href="https://kuhungio.me/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"> 数据挖掘/机器学习 </a>
                        
                </span>

                |
                <a href="#gitalk-container" itemprop="discussionUrl">
                    <span class="gitalk-comment-count" itemprop="commentCount"></span>
                </a>
                条评论
        </div>
    </header>

        <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title"></h2>
  
  <div class="post-toc-content always-active">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#测试新算法的技巧">测试新算法的技巧</a></li>
    <li><a href="#快速上手新任务的技巧">快速上手新任务的技巧</a></li>
    <li><a href="#将一个问题转化为强化学习的技巧">将一个问题转化为强化学习的技巧</a></li>
    <li><a href="#复现论文">复现论文</a></li>
    <li><a href="#对于训练中的一些处理">对于训练中的一些处理</a></li>
    <li><a href="#总的训练策略">总的训练策略</a></li>
    <li><a href="#策略优化诊断">策略优化诊断</a></li>
    <li><a href="#q-learning-策略">Q-Learning 策略</a></li>
  </ul>
</nav>
  </div>
</div>

<script type="text/javascript">
  window.onload = function () {
    var fix = $('.post-toc');
    var end = $('.post-comment');
    var fixTop = fix.offset().top, fixHeight = fix.height();
    var endTop, miss;
    var offsetTop = fix[0].offsetTop;

    $(window).scroll(function () {
      var docTop = Math.max(document.body.scrollTop, document.documentElement.scrollTop);

      if (end.length > 0) {
        endTop = end.offset().top;
        miss = endTop - docTop - fixHeight;
      }

      if (fixTop < docTop) {
        fix.css({ 'position': 'fixed' });
        if ((end.length > 0) && (endTop < (docTop + fixHeight))) {
          fix.css({ top: miss });
        } else {
          fix.css({ top: 0 });
        }
      } else {
        fix.css({ 'position': 'absolute' });
        fix.css({ top: offsetTop });
      }
    })
  }
</script>

    <div class="post-content">
        

        
            
        

        
        

          
          
          

          
          
          

          <h1 id="深度强化学习技巧-hacks-for-training-deep-rl">深度强化学习技巧 hacks for training deep RL</h1>
<p>这是一篇旧文，<a href="http://joschu.net/?utm_source=kuhungio">John Schulman</a> 《深度增强学习研究基础》演讲(Aug 2017)中记录的 tricks。近日重看，发现有些东西在工程中是通用的，值得一读。  </p>
<h2 id="测试新算法的技巧">测试新算法的技巧</h2>
<ol>
<li>简化问题，使用低维变量。</li>
</ol>
<ul>
<li>使用类似只有角度和速度两个变量的 <a href="https://gym.openai.com/envs/Pendulum-v0">Pendulum problem</a> 问题。
<ul>
<li>这样做方便将目标函数、算法的最终状态以及算法的迭代情况可视化出来。</li>
<li>当出现问题时，更容易将出问题的点直观的表达（比如目标函数是否够平滑等问题）。
   </li>
</ul>
</li>
</ul>
<ol start="2">
<li>构造一个 demo 来测试你的算法</li>
</ol>
<ul>
<li>比如：对于一个分层强化学习算法，你应该构造一个算法可以直观学习到分层的问题。
<ul>
<li>这样能够轻易地发现那里出了问题。</li>
<li>注意：不要在这样的小问题上过分的尝试。</li>
</ul>
</li>
</ul>
<ol start="3">
<li>在熟悉的场景中测试</li>
</ol>
<ul>
<li>随着时间的推移，你将能预估训练所需的时间。</li>
<li>明白你的奖赏是如何变化的。</li>
<li>能够设定一个基线，以便让你知道相对过去改进了多少。</li>
<li>作者使用他的 hpper robot，因为他知道算法应该学多块，以及哪些行为是异常的。</li>
</ul>
<h2 id="快速上手新任务的技巧">快速上手新任务的技巧</h2>
<ol>
<li>简化问题</li>
</ol>
<ul>
<li>从简单的开始，直到回到问题。</li>
<li>途径1： 简化特征空间
<ul>
<li>举例来说，如果你是想从图片（高维空间）中学习，那么你可能先需要处理特征。举个例子：如果你的算法是想标定某个事物的位置，一开始，使用单一的x，y坐标可能会更好。</li>
<li>一旦起步，逐步还原问题直到解决问题。</li>
</ul>
</li>
<li>途径2：简化奖赏函数
<ul>
<li>简化奖赏函数，这样可以有一个更快的反馈，帮助你知道是不是走偏了。</li>
<li>比如：击中时给 robot 记一分。这种情况很难学习，因为在开始于奖赏之前有太多的可能。将击中得分改为距离，这样将提升学习速率、更快迭代。</li>
</ul>
</li>
</ul>
<h2 id="将一个问题转化为强化学习的技巧">将一个问题转化为强化学习的技巧</h2>
<p>可能现实是并不清楚特征是什么，也不清楚奖赏该是什么。或者，问题是什么都还不清楚。</p>
<ol>
<li>第一步：将这个问题使用随机策略可视化出来。</li>
</ol>
<ul>
<li>
<p>看看那些部分吸引了你。</p>
</li>
<li>
<p>如果这个随机策略在某些情况下做了正确的事，那么很大概率，强化学习也可以做到。</p>
<ul>
<li>策略的更新将会发现这里面的行为，并促使稳定下来。</li>
</ul>
</li>
<li>
<p>如果随机策略永远都做不到，那么强化学习也不可能。
   </p>
</li>
</ul>
<ol start="2">
<li>确保可观测</li>
</ol>
<ul>
<li>确保你能够掌控系统，且给 agent 的也是同样的系统环境。
<ul>
<li>举个例子： 亲自查看处理过图片，以确保你没有移出掉关键信息或者是在某种程度上阻碍算法。
 </li>
</ul>
</li>
</ul>
<ol start="3">
<li>确保所有的事物都在合理的尺度
<ul>
<li>经验法则：
<ul>
<li>观测环境： 确保均值为0，方差为1。</li>
<li>奖赏： 如果你能控制它，就把他缩放到一个合理的维度。</li>
<li>在所有的数据上都做同样的处理。</li>
</ul>
</li>
</ul>
</li>
</ol>
<ul>
<li>检查所有的观测环境以及奖赏，以确保没有特别离奇的异常值。
   </li>
</ul>
<ol start="4">
<li>建立一个好的基线</li>
</ol>
<ul>
<li>一开始并不清楚哪些算法会起作用，所以设定一系列的基线（从其他方法）。
<ul>
<li>交叉熵</li>
<li>策略更新</li>
<li>一些类型的 Q-learning 算法: <a href="https://github.com/openai/baselines">OpenAI Baselines</a> 或者 <a href="https://github.com/rll/rllab">RLLab</a></li>
</ul>
</li>
</ul>
<h2 id="复现论文">复现论文</h2>
<p>某些时候（经常的事），复现论文结果特别困难。有如下一些技巧：</p>
<ol>
<li>使用比预计更多的样本。</li>
<li>策略正确，但又不完全正确。</li>
</ol>
<ul>
<li>尝试让模型运行更久一点。</li>
<li>调整超参数以达到公开的效果。</li>
<li>如果想让他在所有数据上都奏效，使用更大的 batch sizes。
<ul>
<li>如果 batch size 太小，噪声将会压过真正有用的信号。</li>
<li>比如： TRPO，作者使用了一个特别小的 batch size，然后不得不训练10万次迭代。</li>
<li>对于 DQN，最好的参数：一万次迭代，10亿左右的缓存存储。</li>
</ul>
</li>
</ul>
<h2 id="对于训练中的一些处理">对于训练中的一些处理</h2>
<p>检查你的训练是否正常：</p>
<ol>
<li>检查每个超参数的鲁棒性</li>
</ol>
<ul>
<li>如果一个算法太过敏感，那么它可能不太鲁棒，并且不容乐观。</li>
<li>有些时候，某个策略生效，可能仅仅是因为巧合而已，它并不足以推广。</li>
</ul>
<ol start="2">
<li>关注优化过程是否正常的指标</li>
</ol>
<ul>
<li>变动情况</li>
<li>关注目标函数是否正确
<ul>
<li>是否预测正确？</li>
<li>它的返回是否正确？</li>
<li>每次的更新有多大？</li>
</ul>
</li>
<li>一些标准的深度神经网络的诊断方法</li>
</ul>
<ol start="3">
<li>有一套能够连续记录代码的系统</li>
</ol>
<ul>
<li>有成型的规则。</li>
<li>注意过去所作的所有尝试。
<ul>
<li>有些时候，我们关注一个问题，最后却把注意力放在了其他问题上。</li>
<li>对于一个问题，很容易过拟合。</li>
</ul>
</li>
<li>有一大推之前时不时测试的基准。</li>
</ul>
<ol start="4">
<li>有时候会觉得系统运行正常，但很可能只是巧合。</li>
</ol>
<ul>
<li>例如： 有3套算法处理7个任务，可能会有一个算法看起来能很好地处理所有问题，但实际上，它们不过是一套算法，不同的仅仅是随机种子而已。</li>
</ul>
<ol start="5">
<li>使用不同的随机种子</li>
</ol>
<ul>
<li>运行多次取平均。</li>
<li>在多个种子的基础上运行多次。
<ul>
<li>如果不这样做，那你很有可能是过拟合了。</li>
</ul>
</li>
</ul>
<ol start="6">
<li>额外的算法修改可能并不重要</li>
</ol>
<ul>
<li>大部分的技巧都是在正则化处理某些对象，或者是在改进你的优化算法。</li>
<li>许多技巧有相同的效果&hellip;&hellip;所以，你可以通过移除它们，来简化你的算法（很关键）。</li>
</ul>
<ol start="7">
<li>简化你的算法</li>
</ol>
<ul>
<li>将会取得更好的泛化效果。
   </li>
</ul>
<ol start="8">
<li>自动化你的实验</li>
</ol>
<ul>
<li>不要整天盯着你的代码读取和写入数据。</li>
<li>将实验部署在云端并分析结果。</li>
<li>追踪实验与结果的框架：
<ul>
<li>大部分使用 iPython notebooks。</li>
<li>数据库对于存储结果来说不是很重要。</li>
</ul>
</li>
</ul>
<h2 id="总的训练策略">总的训练策略</h2>
<ol>
<li>数据的白化与标准化（一开始就这样做，对所有数据）</li>
</ol>
<ul>
<li>
<p>训练数据（Obervation）：</p>
<ul>
<li>计算均值与标准差，然后做标准化转变。</li>
<li>对于全体数据（不仅仅是当前的数据）。
<ul>
<li>至少它减轻了随时间的变化波动情况。</li>
<li>如果不断地改变对象的话，可能会使优化器迷糊。</li>
<li>缩放（仅最近的数据）意味着你的优化器不知道这个情况，训练将会失败。</li>
</ul>
</li>
</ul>
</li>
<li>
<p>奖赏（Rewards）：</p>
<ul>
<li>缩放但不要转变对象。
<ul>
<li>影响 agent 的继续下去的可能。</li>
<li>将会改变问题（你想让它持续多久）。</li>
</ul>
</li>
</ul>
</li>
<li>
<p>归一化目标（Standardize targets）</p>
<ul>
<li>与 rewards 相同。</li>
</ul>
</li>
<li>
<p>PCA 白化</p>
<ul>
<li>可以起作用。</li>
<li>开始时的时候要看看它是否真的对神经网络起作用。</li>
<li>大规模的缩放（-1000，1000）到（-0.001，0.001）必然会使学习减缓。</li>
</ul>
</li>
</ul>
<ol start="2">
<li>衰减因子的调参</li>
</ol>
<ul>
<li>判断分配多少权重。</li>
<li>举个例子：如果因子是0.99，那么你将忽略100步以前的事情，这意味着某种程度的短视。
<ul>
<li>最好是看看对应多少的现实时间。
<ul>
<li>直觉上，我们通常在强化学习中离散时间。</li>
<li>换句话说，100步是指3秒前吗？</li>
<li>在这个过程中发生了什么？</li>
</ul>
</li>
</ul>
</li>
<li>如果是用于 fx 估计的策略更新 TD 算法，gamma 可以选择靠近1（比如0.999）。
<ul>
<li>算法将非常稳健。</li>
</ul>
</li>
</ul>
<ol start="3">
<li>观察问题是否真的能被离散化处理</li>
</ol>
<ul>
<li>例如：在一个游戏有跳帧。
<ul>
<li>作为一个人类，你能控制还是不能控制。</li>
<li>查看随机情况是怎么样的。
<ul>
<li>离散化程度决定了你的随机布朗运动能有多远。</li>
</ul>
</li>
<li>如果连续性地运动，往往模型会走很远。</li>
<li>选择一个起作用的时间离散化分。</li>
</ul>
</li>
</ul>
<ol start="4">
<li>密切关注返回的 episode</li>
</ol>
<ul>
<li>不仅仅是均值，还包括极大极小值
<ul>
<li>最大返回是你的策略能做到的最好程度</li>
<li>看看你的策略是否工作正常？</li>
</ul>
</li>
<li>查看 episode 时长（有时比 episode reward 更为重要）
<ul>
<li>在一场游戏中你场场都输，从未赢过，但是 episode 时长可以告诉你是不是输得越来越慢</li>
<li>一开始可能看见 episode 时长有改进，但 reward 无反应</li>
</ul>
</li>
</ul>
<h2 id="策略优化诊断">策略优化诊断</h2>
<ol>
<li>仔仔细细地观察 entropy</li>
</ol>
<ul>
<li>action 层面的 entropy
<ul>
<li>留意状态空间 entropy 的变化，虽然没有好的计量办法</li>
</ul>
</li>
<li>如果快速地下降，策略很快便会固定然后失效</li>
<li>如果没有下降，那么这个策略可能不是很好，因为它是随机的</li>
<li>通过以下方式补救：
<ul>
<li>KL penalty
- 使 entropy 远离快速下降</li>
<li>增加 entropy bonus</li>
</ul>
</li>
<li>如何测量 entropy
<ul>
<li>对于大部分策略，可以采用分析式方法
<ul>
<li>对于连续变量，通常使用高斯分布，这样可以通过微分计算 entropy</li>
</ul>
</li>
</ul>
</li>
</ul>
<ol start="2">
<li>观察 KL 散度</li>
</ol>
<ul>
<li>观察 KL 散度的更新尺度</li>
<li>例如：
<ul>
<li>如果 KL 是0.01，那它是相当小的。</li>
<li>如果是10，那它又非常的大。</li>
</ul>
</li>
</ul>
<ol start="3">
<li>通过基线解释方差</li>
</ol>
<ul>
<li>查看价值函数是不是好的预测或者给予回报
<ul>
<li>如果是负数，可能是过拟合了或者是噪声
<ul>
<li>可能需要超参数调节
 </li>
</ul>
</li>
</ul>
</li>
</ul>
<ol start="4">
<li>初始化策略</li>
</ol>
<ul>
<li>非常重要（比监督学习更重要）</li>
<li>最后一层取0或者是一个很小的值来最大化 entropy
<ul>
<li>初始最大化随机 exploration</li>
</ul>
</li>
</ul>
<h2 id="q-learning-策略">Q-Learning 策略</h2>
<ol>
<li>留意 replay buffer 存储的使用情况</li>
</ol>
<ul>
<li>你可能需要一个非常大的 buffer，依代码而定</li>
</ul>
<ol start="2">
<li>
<p>手边常备 learing rate 表</p>
</li>
<li>
<p>如果收敛很慢或者一开始有一个很慢的启动</p>
</li>
</ol>
<ul>
<li>保持耐心，DQNs 收敛非常慢</li>
</ul>
<p><em>Translated by <a href="https://github.com/kuhung">kuhung</a> 2017/08/29</em></p>
<hr>
<p><figure><img src="/images/ring.svg" data-sizes="auto" data-src="/images/deepRL/deepRL.jpg" alt="" class="lazyload"><figcaption class="image-caption"></figcaption></figure></p>
<blockquote>
<p>关注微信公众号【谷粒先生】，回复【强化学习】获取 PPT 原版</p>
</blockquote>

    </div>

    <div class="post-copyright">
            
            <p class="copyright-item">
                <span>Author:</span>
                <span>谷粒 </span>
                </p>
            

            
            <p class="copyright-item">
                    <span>Link:</span>
                    <a href=https://kuhungio.me/2019/training_rl_systems_hacks/>
                        <script>
                            document.write(decodeURI(location.origin + location.pathname))
                        </script>
                    </a>
            </p>
            
            
            <p class="copyright-item lincese">
                本文采用<a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/" target="_blank">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可
            </p>
            
    </div>


    <div class="post-tags">
        
            <section>
            <i class="iconfont icon-tag"></i>Tag(s):
            
            <span class="tag"><a href="https://kuhungio.me/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">
                    #强化学习</a></span>
            
            <span class="tag"><a href="https://kuhungio.me/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">
                    #深度学习</a></span>
            
            </section>
        
        <section>
                <a href="javascript:window.history.back();">back</a></span> ·
                <span><a href="https://kuhungio.me">home</a></span>
        </section>
    </div>

    <div class="post-nav">
        
        <a href="https://kuhungio.me/2019/flask_vue_ml/" class="prev" rel="prev" title="机器学习建模与部署--以垃圾消息识别为例"><i class="iconfont icon-left"></i>&nbsp;机器学习建模与部署--以垃圾消息识别为例</a>
        
        
        <a href="https://kuhungio.me/2019/book-list-for-ds/" class="next" rel="next" title="数据从业者必读的7本书 Booklist for DE">数据从业者必读的7本书 Booklist for DE&nbsp;<i class="iconfont icon-right"></i></a>
        
    </div>
</article>
          
<div class="post-comment"><div onclick="showDisqus();" id="disqus_title" class="disqus_title">显示 Disqus 评论</div><div id="gitalk-container" class="gitalk-container"></div>
    <link rel="stylesheet" href="/lib/gitalk/gitalk-1.2.2.min.css">
    <script src="/lib/gitalk/gitalk-1.2.2.min.js"></script>
    <script type="text/javascript">
      var gitalk = new Gitalk({
        id: 'e994d9a0e2bbf8744403b76171d4b67d',
        title: '深度强化学习技巧 hacks for training deep RL',
        clientID: '7e5b8bd7063e5315e349',
        clientSecret: 'd6bd237e0a078b7e343775a47533d7b95d364899',
        repo: 'kuhung.github.io',
        owner: 'kuhung',
        admin: ['kuhung'],
        body: decodeURI(location.href)
      });
      gitalk.render('gitalk-container');
    </script>
    <noscript>Please enable JavaScript to view the
      <a href="https://github.com/gitalk/gitalk">comments powered by gitalk.</a>
    </noscript><div id="disqus_thread"></div>
    <script type="text/javascript">
    function showDisqus() {
      

      
      
      
      

      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = false;
      var disqus_shortname = 'kuhungio';
      dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);

      window.location.hash = "#disqus_thread";
    }
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div>

          </div>
		   </main>
      <footer class="footer">
    <div class="copyright">
        &copy;
        
        <span itemprop="copyrightYear">2016 - 2023</span>
        
        <span class="with-love">
    	 <i class="iconfont icon-love"></i>
         </span>
         
            <span class="author" itemprop="copyrightHolder"><a href="https://kuhungio.me">谷粒</a> | </span>
         

         
        <span>Powered by <a href="https://gohugo.io/" target="_blank" rel="external nofollow">Hugo</a> & <a href="https://github.com/liuzc/leaveit" target="_blank" rel="external nofollow">LeaveIt</a></span>
        
    </div>
    <script>
      const currentTheme = window.localStorage && window.localStorage.getItem('theme')
      const isDark = currentTheme === 'dark'
      if (isDark) {
        document.body.classList.add('dark-theme')
      }
    </script>
</footer>












    
     <link href="//cdn.bootcdn.net/ajax/libs/lightgallery/1.10.0/css/lightgallery.min.css" rel="stylesheet">  
      
     <script src="/js/vendor_gallery.min.js" async="" ></script>
    
  



     </div>
  </body>
</html>
