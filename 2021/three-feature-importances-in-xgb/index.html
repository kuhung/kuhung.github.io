<!DOCTYPE html>
<html lang="zh-CN">
  <head>
  <meta http-equiv="content-type" content="text/html;charset=utf-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="robots" content="noodp"/>
  <meta name="author" content="谷粒">
  <meta name="description" content="数据挖掘/机器学习工程师">
  <meta name="keywords" content="华中科技大学,谷粒,谷粒粒,数据挖掘,网易游戏,米哈游,广告投放,用户分析,增长黑客,开发者,极客,代码,开源,Developer,Programmer,Coder,Geek,DataScientist">
  
  
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-76017508-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
  <link rel="prev" href="https://kuhungio.me/2021/what-i-learned-from-little-game-xiao/" />
  <link rel="next" href="https://kuhungio.me/2021/performance-coaching/" />
  <link rel="canonical" href="https://kuhungio.me/2021/three-feature-importances-in-xgb/" />
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">
  <title>
       
       
           Xgboost 三种特征重要性计算方法对比与扩展 | 谷粒的博客
       
  </title>
  <meta name="title" content="Xgboost 三种特征重要性计算方法对比与扩展 | 谷粒的博客">
    
  
  <link rel="stylesheet" href="/font/iconfont.css">
  <link rel="stylesheet" href="/css/main.min.css">


  
  
 

<script type="application/ld+json">
 "@context" : "http://schema.org",
    "@type" : "BlogPosting",
    "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "https:\/\/kuhungio.me"
    },
    "articleSection" : "posts",
    "name" : "Xgboost 三种特征重要性计算方法对比与扩展",
    "headline" : "Xgboost 三种特征重要性计算方法对比与扩展",
    "description" : "简单描述 实际发布 最后一次修改 xgb 特征重要性计算方法及使用场景 2021-03-08 2021-03-09 特征重要性 作用与来源 特征重要性，我们一般用来观察不同特征的贡献度。排名靠前的，我们自然而然地认为，它是重要的。\n这一思路，通常被用来做特征筛选。剔除贡献度不高的尾部特征，增强模型的鲁棒性的同时，起到特征降维的作用。\n另一个方面，则是用来做模型的可解释性。我们期望的结果是：重要的特征是符合业务直觉的；符合业务直觉的特征排名靠前。\n在实际操作中，我们一般用树模型的分类节点做文章。常用的就是 XGB 和其他一般树模型。\nXGB 遇到的问题 XGB 很方便，不仅是比赛的大杀器，甚至贴心的内置了重要性函数。但在实际使用过程中，常常陷入迷思。\n有如下几个点的顾虑：\n这些特征重要性是如何计算得到的？ 为什么特征重要性不同？ 什么情况下采用何种特征重要性合适？ 今天我们就借这篇文章梳理一下。\nXGB 中常用的三种特征重要性计算方法，以及它的使用场景。除此之外，再看两个第三方的特征重要性计算方法，跳出内置函数，思考其中的差异。\n最后回到类似的树模型特征计算方法，进行特征重要性的一般方法总结。\n以下场景非特殊说明，均针对 python 包体下的 xgb 和sklearn。\nXGB 内置的三种特征重要性计算方法1 weight xgb.plot_importance 这是我们常用的绘制特征重要性的函数方法。其背后用到的贡献度计算方法为weight。\n‘weight’ - the number of times a feature is used to split the data across all trees. 简单来说，就是在子树模型分裂时，用到的特征次数。这里计算的是所有的树。这个指标在R包里也被称为**frequency**2。\ngain model.feature_importances_ 这是我们调用特征重要性数值时，用到的默认函数方法。其背后用到的贡献度计算方法为gain。\n‘gain’ - the average gain across all splits the feature is used in.",
    "inLanguage" : "zh-CN",
    "author" : "谷粒",
    "creator" : "谷粒",
    "publisher": "谷粒",
    "accountablePerson" : "谷粒",
    "copyrightHolder" : "谷粒",
    "copyrightYear" : "2021",
    "datePublished": "2021-03-08 21:09:31 \u002b0800 CST",
    "dateModified" : "2021-03-08 21:09:31 \u002b0800 CST",
    "url" : "https:\/\/kuhungio.me\/2021\/three-feature-importances-in-xgb\/",
    "wordCount" : "236",
    "keywords" : [ "数据挖掘","总结","特征工程", "谷粒的博客"]
}
</script>

</head>

  


  <body class="">
    <div class="wrapper">
        <nav class="navbar">
    <div class="container">
        <div class="navbar-header header-logo">
        	<a href="javascript:void(0);" class="theme-switch"><i class="iconfont icon-xihuan"></i></a>&nbsp;<a href="https://kuhungio.me">谷粒的博客</a>
        </div>
        <div class="menu navbar-right">
                
                
                <a class="menu-item" href="/posts/" title="">历史博文</a>
                
                <a class="menu-item" href="/tags/mindmap/" title="">思维导图</a>
                
                <a class="menu-item" href="/about/" title="关于我">关于我</a>
                
        </div>
    </div>
</nav>
<nav class="navbar-mobile" id="nav-mobile" style="display: none">
     <div class="container">
        <div class="navbar-header">
            <div>  <a href="javascript:void(0);" class="theme-switch"><i class="iconfont icon-xihuan"></i></a>&nbsp;<a href="https://kuhungio.me">谷粒的博客</a></div>
            <div class="menu-toggle">
                <span></span><span></span><span></span>
            </div>
        </div>
     
          <div class="menu" id="mobile-menu">
                
                
                <a class="menu-item" href="/posts/" title="">历史博文</a>
                
                <a class="menu-item" href="/tags/mindmap/" title="">思维导图</a>
                
                <a class="menu-item" href="/about/" title="关于我">关于我</a>
                
        </div>
    </div>
</nav>
    	 <main class="main">
          <div class="container">
      		
<article class="post-warp" itemscope itemtype="http://schema.org/Article">
    <header class="post-header">
        <h1 class="post-title" itemprop="name headline">Xgboost 三种特征重要性计算方法对比与扩展</h1>
        <div class="post-meta">
                Written by <a itemprop="name" href="https://kuhungio.me" rel="author">谷粒</a> with ♥
                <span class="post-time">
                on <time datetime=2021-03-08 itemprop="datePublished">March 8, 2021</time>
                </span>
                in
                <i class="iconfont icon-folder"></i>
                <span class="post-category">
                        <a href="https://kuhungio.me/categories/%E6%8A%80%E6%9C%AF%E6%8A%80%E5%B7%A7/"> 技术技巧 </a>
                        <a href="https://kuhungio.me/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"> 数据挖掘/机器学习 </a>
                        
                </span>

                |
                <a href="#gitalk-container" itemprop="discussionUrl">
                    <span class="gitalk-comment-count" itemprop="commentCount"></span>
                </a>
                条评论
        </div>
    </header>

        <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title"></h2>
  
  <div class="post-toc-content always-active">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#特征重要性">特征重要性</a>
      <ul>
        <li><a href="#作用与来源">作用与来源</a></li>
        <li><a href="#xgb-遇到的问题">XGB 遇到的问题</a></li>
      </ul>
    </li>
    <li><a href="#xgb-内置的三种特征重要性计算方法1">XGB 内置的三种特征重要性计算方法</a>
      <ul>
        <li><a href="#weight">weight</a></li>
        <li><a href="#gain">gain</a></li>
        <li><a href="#cover">cover</a></li>
      </ul>
    </li>
    <li><a href="#使用场景">使用场景</a></li>
    <li><a href="#其他重要性计算方法5">其他重要性计算方法</a>
      <ul>
        <li><a href="#permutation">permutation</a></li>
        <li><a href="#shap">shap</a></li>
      </ul>
    </li>
    <li><a href="#其他模型的特征重要性计算方法">其他模型的特征重要性计算方法</a></li>
    <li><a href="#特征重要性小结">特征重要性小结</a></li>
  </ul>
</nav>
  </div>
</div>

<script type="text/javascript">
  window.onload = function () {
    var fix = $('.post-toc');
    var end = $('.post-comment');
    var fixTop = fix.offset().top, fixHeight = fix.height();
    var endTop, miss;
    var offsetTop = fix[0].offsetTop;

    $(window).scroll(function () {
      var docTop = Math.max(document.body.scrollTop, document.documentElement.scrollTop);

      if (end.length > 0) {
        endTop = end.offset().top;
        miss = endTop - docTop - fixHeight;
      }

      if (fixTop < docTop) {
        fix.css({ 'position': 'fixed' });
        if ((end.length > 0) && (endTop < (docTop + fixHeight))) {
          fix.css({ top: miss });
        } else {
          fix.css({ top: 0 });
        }
      } else {
        fix.css({ 'position': 'absolute' });
        fix.css({ top: offsetTop });
      }
    })
  }
</script>

    <div class="post-content">
        

        
            
        

        
        

          
          
          

          
          
          

          <table>
<thead>
<tr>
<th>简单描述</th>
<th>实际发布</th>
<th>最后一次修改</th>
</tr>
</thead>
<tbody>
<tr>
<td>xgb 特征重要性计算方法及使用场景</td>
<td>2021-03-08</td>
<td>2021-03-09</td>
</tr>
</tbody>
</table>
<h2 id="特征重要性">特征重要性</h2>
<h3 id="作用与来源">作用与来源</h3>
<p>特征重要性，我们一般用来观察不同特征的贡献度。排名靠前的，我们自然而然地认为，它是重要的。</p>
<p>这一思路，通常被用来做<strong>特征筛选</strong>。剔除贡献度不高的尾部特征，增强模型的鲁棒性的同时，起到特征降维的作用。</p>
<p>另一个方面，则是用来做<strong>模型的可解释性</strong>。我们期望的结果是：重要的特征是符合业务直觉的；符合业务直觉的特征排名靠前。</p>
<p>在实际操作中，我们一般用树模型的分类节点做文章。常用的就是 XGB 和其他一般树模型。</p>
<h3 id="xgb-遇到的问题">XGB 遇到的问题</h3>
<p>XGB 很方便，不仅是比赛的大杀器，甚至贴心的内置了重要性函数。但在实际使用过程中，常常陷入迷思。</p>
<p>有如下几个点的顾虑：</p>
<ol>
<li>这些特征重要性是如何计算得到的？</li>
<li>为什么特征重要性不同？</li>
<li>什么情况下采用何种特征重要性合适？</li>
</ol>
<hr>
<p>今天我们就借这篇文章梳理一下。</p>
<p>XGB 中常用的三种特征重要性计算方法，以及它的使用场景。除此之外，再看两个第三方的特征重要性计算方法，跳出内置函数，思考其中的差异。</p>
<p>最后回到类似的树模型特征计算方法，进行特征重要性的一般方法总结。</p>
<p>以下场景非特殊说明，均针对 python 包体下的 xgb 和sklearn。</p>
<h2 id="xgb-内置的三种特征重要性计算方法1">XGB 内置的三种特征重要性计算方法<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></h2>
<h3 id="weight">weight</h3>
<p><code>xgb.plot_importance</code> 这是我们常用的绘制特征重要性的函数方法。其背后用到的贡献度计算方法为<code>weight</code>。</p>
<ul>
<li>‘weight’ - the number of times a feature is used to split the data across all trees.</li>
</ul>
<p>简单来说，就是在子树模型分裂时，用到的特征次数。这里计算的是所有的树。这个指标在R包里也被称为**<code>frequency</code>**<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>。</p>
<h3 id="gain">gain</h3>
<p><code>model.feature_importances_</code> 这是我们调用特征重要性数值时，用到的默认函数方法。其背后用到的贡献度计算方法为<code>gain</code>。</p>
<ul>
<li>‘gain’ - the average gain across all splits the feature is used in.</li>
</ul>
<p>gain 是信息增益的泛化概念。这里是指，节点分裂时，该特征带来信息增益（目标函数）优化的平均值。</p>
<h3 id="cover">cover</h3>
<p><code>model = XGBRFClassifier(importance_type = 'cover')</code> 这个计算方法，需要在定义模型时定义。之后再调用<code>model.feature_importances_</code> 得到的便是基于<code>cover</code>得到的贡献度。</p>
<ul>
<li>‘cover’ - the average coverage across all splits the feature is used in.</li>
</ul>
<p>cover 形象来说，就是树模型在分裂时，特征下的叶子结点涵盖的样本数除以特征用来分裂的次数。分裂越靠近根部，cover 值越大。</p>
<h2 id="使用场景">使用场景</h2>
<p>weight 将给予数值特征更高的值，因为它的变数越多，树分裂时可切割的空间越大。所以这个指标，会掩盖掉重要的枚举特征。</p>
<p>gain 用到了熵增的概念，它可以方便的找出最直接的特征。即如果某个特征的下的0，在label下全是0，则这个特征一定会排得靠前。</p>
<p>cover 对于枚举特征，会更友好。同时，它也没有过度拟合目标函数，不受目标函数的量纲影响。</p>
<p>调用它们的方式如下<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Available importance_types = [&#39;weight&#39;, &#39;gain&#39;, &#39;cover&#39;, &#39;total_gain&#39;, &#39;total_cover&#39;]</span>
</span></span><span style="display:flex;"><span>f <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;gain&#39;</span>
</span></span><span style="display:flex;"><span>XGBClassifier<span style="color:#f92672">.</span>get_booster()<span style="color:#f92672">.</span>get_score(importance_type<span style="color:#f92672">=</span> f)
</span></span></code></pre></div><p>举个例子，我们来做西瓜分类任务。西瓜有颜色、重量、声音、品种、产地等各类特征。其中，生活（业务）经验告诉我们，声音响的瓜甜。我们构建了这么一个模型，判断西瓜甜不甜。输出三类特征重要性。这时会看到一些矛盾的现象<sup id="fnref1:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>。</p>
<p><figure><img src="/images/ring.svg" data-sizes="auto" data-src="https://miro.medium.com/max/1000/1*qdvN5WQhN33hO4wWG7RtDw.png" alt="be-careful-when-interpreting-your-features-importance-in-xgboost" class="lazyload"><figcaption class="image-caption">be-careful-when-interpreting-your-features-importance-in-xgboost</figcaption></figure></p>
<p>在 <code>weight</code> 解释下，声音这类枚举值特征并不会很靠前。反而是重量这类连续特征会靠前。为什么会这样子，是因为连续特征提供了<strong>更多的切分状态空间</strong>，这样的结果，势必导致它在树模型的分裂点上占据多个位置。与此同时，<strong>重要的枚举值特征靠后</strong>了。而且我们还能预见的是，同一个量纲下，上下界越大的特征，更有可能靠前。</p>
<p>如何解决这个矛盾点，那就是采用 <code>gain</code> 或者 <code>cover</code> 方法。因为声音这个特征，必然能带来更多的信息增益，减少系统的熵，所以它在信息增益数值上，一定是一个大值。在树模型的分类节点上，也一定是优先作为分裂点，靠近根部的。</p>
<p>在实践中，也会发现，<code>gain</code> 排出来的顺序的<strong>头尾部值差距较大</strong>，这是因为信息增益计算时，后续的优化可能都不是一个量级。类似于神经网络在优化损失函数时，后续的量纲可能是十倍、百倍的差异。所以，综上而言，如果<strong>有下游业务方</strong>，更建议用 <code>cover</code> 的特征重要性计算方法。当然，如果是单纯的模型调优，<code>gain</code> 能指出最重要的特征。这些特征，某些场景下还能总结成硬规则。</p>
<h2 id="其他重要性计算方法5">其他重要性计算方法<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup></h2>
<p>除了上述内置的重要性计算方法外，还有其他第三方计算方式。</p>
<p>这里介绍两种，一个是 <a href="https://scikit-learn.org/stable/modules/permutation_importance.html#permutation-importance">permutation importance</a><sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup><sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>，另一个是 shap<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>。</p>
<h3 id="permutation">permutation</h3>
<p>Permutation 的逻辑<sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>是：如果这个特征很重要，那么我们打散所有样本中的该特征，则最后的优化目标将折损。这里的折损程度，就是特征的重要程度。</p>
<p>由于其计算依赖单一特征，所以对非线性模型更友好。同时，如果特征中存在多重共线性，共线的特征重要性都将非常靠后。这是因为混淆单一特征，不影响另一个特征的贡献。这样的结果是，即使特征很重要，也会排得很靠后。</p>
<p>针对多重共线特征，<a href="https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance_multicollinear.html#handling-multicollinear-features">sklearn</a> 文档中提到了一种解决方法<sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>：计算特征间的 spearman rank-order correlations，取得每一组中的头部特征，再进行特征重要性计算。这种方法，实际上是在解决特征共线的问题。</p>
<h3 id="shap">shap</h3>
<p>另一个特征重要性计算方法 shap，在之前的文章：<a href="/interpretable-machine-learning.md">机器学习模型的两种解释方法</a><sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>有过介绍。其计算方法，利用了博弈论的知识，即特征的边际递减效应。通俗来说，就是轮流去掉每一个特征，算出剩下特征的贡献情况，以此来推导出被去除特征的边际贡献。该方法是目前唯一的逻辑严密的特征解释方法（但实际使用体感一般，详情可参见上篇文章）。</p>
<h2 id="其他模型的特征重要性计算方法">其他模型的特征重要性计算方法</h2>
<p>对于同样的树模型，randomforest 和 gbdt，同样也有内置的特征重要性。</p>
<p>randomforest 使用 <code>rf.feature_importances_</code> 得到特征重要性<sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>。其中，分类任务计算的是 gini 不纯度/信息熵。回归任务计算的是树的方差。</p>
<p>这种基于<strong>不纯度</strong>（Mean Decrease in Impurity）的方法，实际上会有两个问题存在：一：会给予变量空间更大的特征更多关注，而二分类特征则会靠后。二：结果的拟合是基于训练集的，存在过拟合风险，没有验证集数据做验证。</p>
<p>针对上述的问题，建议通过out-of-bag（OOB）方法<sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup><sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup>，在剩下数据上做验证，结合 permutation<sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup> 计算特征重要性。</p>
<p>此外，gbdt 也是基于<strong>不纯度</strong>计算的特征重要性，不过其在单棵树上，是回归树<sup id="fnref:15"><a href="#fn:15" class="footnote-ref" role="doc-noteref">15</a></sup>，所以不是基于gini系数，而是 MSE 或 MAE。</p>
<p>至于为什么它们同为树模型，且都是基于不纯度计算的重要性，但结果不同。主要有两个，一个是它们的树结构不同；第二个则是，它们的优化对象不同。</p>
<h2 id="特征重要性小结">特征重要性小结</h2>
<p>总结起来，我们可以发现，这里特征重要性的计算，其实分为了两类。一类是<strong>基于优化过程</strong>的，如树模型都在采用的不纯度。另一类则是<strong>基于外部结果</strong>，通过调整或引入变量，观察对目标的影响。</p>
<p>目前的趋势是，不纯度的局限被大家认识到，越来越多地采用外部结果的方式，来保证特征重要性的一致性和稳定性<sup id="fnref:16"><a href="#fn:16" class="footnote-ref" role="doc-noteref">16</a></sup>。</p>
<p>对于不稳定性，可以观察到的一个现象是，新特征的加入，很可能打乱（而不是插入）头部特征的排名。如何减缓这个现象？打乱，其实说明了两个问题。一：特征重要性排名的工具有问题，不具有鲁棒性，应当采用多种特征重要性工具对比。二：特征间存在相关性，树模型在解释时，也应当进行相关性剔除。</p>
<hr>
<p>三种直观的外部检验方法：</p>
<ol>
<li>将特征随机打散，其实就是 permutation 的计算方式。</li>
<li>引入随机变量列，观察其他特征相对于它的位置排序。</li>
<li>去掉特征<sup id="fnref1:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>，观察目标函数的升降情况。</li>
</ol>
<p><a href="/about/">关于作者</a></p>
<p>参考资料：</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p><a href="https://stackoverflow.com/questions/37627923/how-to-get-feature-importance-in-xgboost">How to get feature importance in xgboost?  &ndash;  stackoverflow</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p><a href="https://datascience.stackexchange.com/questions/12318/how-to-interpret-the-output-of-xgboost-importance">how-to-interpret-the-output-of-xgboost-importance  &ndash;  stackexchange</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p><a href="https://towardsdatascience.com/be-careful-when-interpreting-your-features-importance-in-xgboost-6e16132588e7">be-careful-when-interpreting-your-features-importance-in-xgboost</a>&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p><a href="https://stackoverflow.com/questions/57323555/get-individual-features-importance-with-xgboost">Get individual features importance with XGBoost  &ndash;  stackoverflow</a>&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p><a href="https://scikit-learn.org/stable/modules/permutation_importance.html#permutation-importance">https://scikit-learn.org/stable/modules/permutation_importance.html#permutation-importance</a>&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p><a href="http://rasbt.github.io/mlxtend/user_guide/evaluate/feature_importance_permutation/">http://rasbt.github.io/mlxtend/user_guide/evaluate/feature_importance_permutation/</a>&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p><a href="https://shap.readthedocs.io/en/latest/">https://shap.readthedocs.io/en/latest/</a>&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p><a href="https://www.kaggle.com/dansbecker/permutation-importance#Interpreting-Permutation-Importances">permutation-importance#Interpreting-Permutation-Importances  &ndash; kaggle</a>&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p><a href="https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance_multicollinear.html#handling-multicollinear-features">plot_permutation_importance_multicollinear.html#handling-multicollinear-features</a>&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p><a href="https://kuhungio.me/2021/interpretable-machine-learning/#shap">https://kuhungio.me/2021/interpretable-machine-learning/#shap</a>&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p><a href="https://towardsdatascience.com/explaining-feature-importance-by-example-of-a-random-forest-d9166011959e">explaining-feature-importance-by-example-of-a-random-forest  &ndash;  towardsdatascience</a>&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p><a href="https://mljar.com/blog/feature-importance-in-random-forest/">https://mljar.com/blog/feature-importance-in-random-forest/</a>&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p><a href="https://stackoverflow.com/questions/18541923/what-is-out-of-bag-error-in-random-forests">https://stackoverflow.com/questions/18541923/what-is-out-of-bag-error-in-random-forests</a>&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p><a href="https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html">Permutation Importance vs Random Forest Feature Importance (MDI)</a>&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:15">
<p><a href="https://zhuanlan.zhihu.com/p/64759172">https://zhuanlan.zhihu.com/p/64759172</a>&#160;<a href="#fnref:15" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:16">
<p><a href="https://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27">Interpretable Machine Learning with XGBoost  &ndash;  towardsdatascience</a>&#160;<a href="#fnref:16" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

    </div>

    <div class="post-copyright">
            
            <p class="copyright-item">
                <span>Author:</span>
                <span>谷粒 </span>
                </p>
            

            
            <p class="copyright-item">
                    <span>Link:</span>
                    <a href=https://kuhungio.me/2021/three-feature-importances-in-xgb/>
                        <script>
                            document.write(decodeURI(location.origin + location.pathname))
                        </script>
                    </a>
            </p>
            
            
            <p class="copyright-item lincese">
                本文采用<a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/" target="_blank">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可
            </p>
            
    </div>


    <div class="post-tags">
        
            <section>
            <i class="iconfont icon-tag"></i>Tag(s):
            
            <span class="tag"><a href="https://kuhungio.me/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/">
                    #数据挖掘</a></span>
            
            <span class="tag"><a href="https://kuhungio.me/tags/%E6%80%BB%E7%BB%93/">
                    #总结</a></span>
            
            <span class="tag"><a href="https://kuhungio.me/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/">
                    #特征工程</a></span>
            
            </section>
        
        <section>
                <a href="javascript:window.history.back();">back</a></span> ·
                <span><a href="https://kuhungio.me">home</a></span>
        </section>
    </div>

    <div class="post-nav">
        
        <a href="https://kuhungio.me/2021/what-i-learned-from-little-game-xiao/" class="prev" rel="prev" title="从一款小游戏收获的三点思考"><i class="iconfont icon-left"></i>&nbsp;从一款小游戏收获的三点思考</a>
        
        
        <a href="https://kuhungio.me/2021/performance-coaching/" class="next" rel="next" title="高绩效教练思维导图">高绩效教练思维导图&nbsp;<i class="iconfont icon-right"></i></a>
        
    </div>
</article>
          
<div class="post-comment"><div onclick="showDisqus();" id="disqus_title" class="disqus_title">显示 Disqus 评论</div><div id="gitalk-container" class="gitalk-container"></div>
    <link rel="stylesheet" href="/lib/gitalk/gitalk-1.2.2.min.css">
    <script src="/lib/gitalk/gitalk-1.2.2.min.js"></script>
    <script type="text/javascript">
      var gitalk = new Gitalk({
        id: '71d18ef393e51afb9fb2cb384cf940c8',
        title: 'Xgboost 三种特征重要性计算方法对比与扩展',
        clientID: '7e5b8bd7063e5315e349',
        clientSecret: 'd6bd237e0a078b7e343775a47533d7b95d364899',
        repo: 'kuhung.github.io',
        owner: 'kuhung',
        admin: ['kuhung'],
        body: decodeURI(location.href)
      });
      gitalk.render('gitalk-container');
    </script>
    <noscript>Please enable JavaScript to view the
      <a href="https://github.com/gitalk/gitalk">comments powered by gitalk.</a>
    </noscript><div id="disqus_thread"></div>
    <script type="text/javascript">
    function showDisqus() {
      

      
      
      
      

      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = false;
      var disqus_shortname = 'kuhungio';
      dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);

      window.location.hash = "#disqus_thread";
    }
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div>

          </div>
		   </main>
      <footer class="footer">
    <div class="copyright">
        &copy;
        
        <span itemprop="copyrightYear">2016 - 2022</span>
        
        <span class="with-love">
    	 <i class="iconfont icon-love"></i>
         </span>
         
            <span class="author" itemprop="copyrightHolder"><a href="https://kuhungio.me">谷粒</a> | </span>
         

         
        <span>Powered by <a href="https://gohugo.io/" target="_blank" rel="external nofollow">Hugo</a> & <a href="https://github.com/liuzc/leaveit" target="_blank" rel="external nofollow">LeaveIt</a></span>
        
    </div>
    <script>
      const currentTheme = window.localStorage && window.localStorage.getItem('theme')
      const isDark = currentTheme === 'dark'
      if (isDark) {
        document.body.classList.add('dark-theme')
      }
    </script>
</footer>












    
     <link href="//cdn.bootcdn.net/ajax/libs/lightgallery/1.10.0/css/lightgallery.min.css" rel="stylesheet">  
      
     <script src="/js/vendor_gallery.min.js" async="" ></script>
    
  



     </div>
  </body>
</html>
