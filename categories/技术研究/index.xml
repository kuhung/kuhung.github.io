<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>技术研究 on 谷粒的博客</title>
    <link>https://kuhung.me/categories/%E6%8A%80%E6%9C%AF%E7%A0%94%E7%A9%B6/</link>
    <description>Recent content in 技术研究 on 谷粒的博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Sat, 24 May 2025 12:15:10 +0800</lastBuildDate><atom:link href="https://kuhung.me/categories/%E6%8A%80%E6%9C%AF%E7%A0%94%E7%A9%B6/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>FastVLM iPhone 安装与实机测评</title>
      <link>https://kuhung.me/2025/fastvlm-iphone-install/</link>
      <pubDate>Sat, 24 May 2025 12:15:10 +0800</pubDate>
      
      <guid>https://kuhung.me/2025/fastvlm-iphone-install/</guid>
      <description>项目简介 FastVLM 是苹果发布的视觉语言模型，可以在iPhone和Mac上离线运行，能够理解图像内容并回答问题。所有预测都在设备本地处理，确保隐私和安全。项目地址：https://github.com/apple/ml-fastvlm
接下来说说实测最关心的性能表现和大小情况：
作者的测试设备为：
MBP M1Pro 16G运行内存 iPhone16Pro A18Pro 8G运行内存 模型版本 下载大小 TTFT（MBP） 内存占用 FastVLM 0.5B (fp16) 1.15G 400ms 2G FastVLM 1.5B (int8) 1.91G 600ms 2.8G FastVLM 7B (int4) 3.85G 1800ms 5G 注意：以上大小均为适配转换后，适用于Apple Silicon的模型。原始模型大小会有出入，实际按指令下载的大小即为表格所述。
作者测试下来，认为该技术是一个初步的技术原型。能够满足基本的视频理解，在终端跑起来仍然会有发热的问题。但瑕不掩瑜，未来在端侧，AI将会有更多的可能性。打包后，最小的安装包大小在2G左右。基本上每个模型都会把MBP的CPU温度干到85度以上，风扇直接到6000转以上。真没见过这么吵的MBP。iPhone16pro的情况也大差不多，能感受到明显的温度升高。不过手头没有测温装置，所以无法给到确切的数字。而在速度方面，基本如上图所示，能够在秒级别以内产出首个Token。iPhone再乘以2-3倍。当然，这个时间也取决于提示词、图片复杂度等其他因素。
对于未来展望部分：调用逻辑和交互上，适当的优化能够降低发热，提升用户体验。无论如何，AI新技术都需要整合到需求中，才会有更大的价值。
准备工作 系统要求 Mac电脑：macOS 15.2+ 并安装Xcode最新版本 iPhone设备：支持iOS 18.2+的设备（建议运行内存在3G以上，iPhone X 及以上） 开发者账号：普通Apple ID即可 项目特性 ✅ 支持iOS (18.2+) 和macOS (15.2+) ✅ 显示每次推理的首Token时间(TTFT) ✅ 完全离线运行，保护隐私安全 ✅ 灵活的提示系统，支持自定义prompt ✅ 三种不同规格的预训练模型 安装步骤 1. 下载源代码 git clone https://github.com/apple/ml-fastvlm.git cd ml-fastvlm 2.</description>
    </item>
    
  </channel>
</rss>
