<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>技术研究 on Kuhung | 谷粒</title>
    <link>https://kuhung.me/categories/%E6%8A%80%E6%9C%AF%E7%A0%94%E7%A9%B6/</link>
    <description>Recent content in 技术研究 on Kuhung | 谷粒</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Sun, 23 Nov 2025 11:11:54 +0800</lastBuildDate><atom:link href="https://kuhung.me/categories/%E6%8A%80%E6%9C%AF%E7%A0%94%E7%A9%B6/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>富士 LUTs 应用到 iPhone 前需要弄清楚的6个问题</title>
      <link>https://kuhung.me/posts/luts-for-iphone/</link>
      <pubDate>Sun, 23 Nov 2025 11:11:54 +0800</pubDate>
      
      <guid>https://kuhung.me/posts/luts-for-iphone/</guid>
      <description>最近富士公开了其经典的十数种 LUT 滤镜，包括著名的 CC 滤镜和 NC 滤镜，使得其他拍摄设备，也能最大限度还原得到富士相机的效果，而不需要购入对应的富士相机。但在将富士LUTs应用到iPhone前，不少人是第一次接触到这一概念，这需要我们花点儿时间，来理解背后的色彩科学。
笔者通过编程方式，结合实际数据和图表，力图深入浅出解释LUT、Log模式以及为什么需要进行色彩空间转换。
1. 什么是 LUT (Look-Up Table) LUT 本质上是一个映射表，就如其英文全称所言。对于 3D LUT，它是一个 RGB 立方体。输入一个 RGB 值 (R, G, B)，查找表，输出一个新的 RGB 值。
LUT 的核心映射逻辑 输入 RGB: 假设视频里有一个像素，颜色是 R=0.5, G=0.5, B=0.5 (中性灰)。 查找 (Look Up): LUT 文件是一个三维数组 (例如 33x33x33)。 我们把 0.0~1.0 的范围划分成 33 个格子。 0.5 刚好对应第 17 个格子索引 (因为 16/32 = 0.5)。 定位: 找到坐标为 (17, 17, 17) 的那个小格子。 读取: 读取这个格子预先存储的 RGB 值。比如，胶片模拟可能会把这个灰变成略带冷调的 (0.48, 0.49, 0.52)。 插值 (Interpolation): 如果输入是 0.</description>
    </item>
    
    <item>
      <title>FastVLM iPhone 安装与实机测评</title>
      <link>https://kuhung.me/posts/fastvlm-iphone-install/</link>
      <pubDate>Sat, 24 May 2025 12:15:10 +0800</pubDate>
      
      <guid>https://kuhung.me/posts/fastvlm-iphone-install/</guid>
      <description>项目简介 FastVLM 是苹果发布的视觉语言模型，可以在iPhone和Mac上离线运行，能够理解图像内容并回答问题。所有预测都在设备本地处理，确保隐私和安全。项目地址：https://github.com/apple/ml-fastvlm
接下来说说实测最关心的性能表现和大小情况：
作者的测试设备为：
MBP M1Pro 16G运行内存 iPhone16Pro A18Pro 8G运行内存 模型版本 下载大小 TTFT（MBP） 内存占用 FastVLM 0.5B (fp16) 1.15G 400ms 2G FastVLM 1.5B (int8) 1.91G 600ms 2.8G FastVLM 7B (int4) 3.85G 1800ms 5G 注意：以上大小均为适配转换后，适用于Apple Silicon的模型。原始模型大小会有出入，实际按指令下载的大小即为表格所述。
作者测试下来，认为该技术是一个初步的技术原型。能够满足基本的视频理解，在终端跑起来仍然会有发热的问题。但瑕不掩瑜，未来在端侧，AI将会有更多的可能性。打包后，最小的安装包大小在2G左右。基本上每个模型都会把MBP的CPU温度干到85度以上，风扇直接到6000转以上。真没见过这么吵的MBP。iPhone16pro的情况也大差不多，能感受到明显的温度升高。不过手头没有测温装置，所以无法给到确切的数字。而在速度方面，基本如上图所示，能够在秒级别以内产出首个Token。iPhone再乘以2-3倍。当然，这个时间也取决于提示词、图片复杂度等其他因素。
对于未来展望部分：调用逻辑和交互上，适当的优化能够降低发热，提升用户体验。无论如何，AI新技术都需要整合到需求中，才会有更大的价值。
准备工作 系统要求 Mac电脑：macOS 15.2+ 并安装Xcode最新版本 iPhone设备：支持iOS 18.2+的设备（建议运行内存在3G以上，iPhone X 及以上） 开发者账号：普通Apple ID即可 项目特性 ✅ 支持iOS (18.2+) 和macOS (15.2+) ✅ 显示每次推理的首Token时间(TTFT) ✅ 完全离线运行，保护隐私安全 ✅ 灵活的提示系统，支持自定义prompt ✅ 三种不同规格的预训练模型 安装步骤 1. 下载源代码 git clone https://github.com/apple/ml-fastvlm.git cd ml-fastvlm 2.</description>
    </item>
    
  </channel>
</rss>
